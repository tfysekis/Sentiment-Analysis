{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tfysekis/Sentiment-Analysis/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAT0GEo28QAQ"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbtMOM8v-gOi"
      },
      "source": [
        "In this project, we will conduct sentiment analysis using the Amazon Fine Food Reviews dataset. This dataset contains over 500,000 customer reviews of food products available on Amazon, making it a rich resource for analyzing customer opinions. Each review includes both the text of the review and a numerical rating (1–5), which we will use to classify sentiment into three categories:\n",
        "\n",
        "- Positive: Ratings of 4 or 5\n",
        "- Neutral: A rating of 3\n",
        "- Negative: Ratings of 1 or 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yUGk3Mc-yzR"
      },
      "source": [
        "## Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8HKb_0W-yaW",
        "outputId": "a75e8e04-4705-4ce5-e648-a2711af49d79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Id   ProductId          UserId                      ProfileName  \\\n",
            "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
            "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
            "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
            "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
            "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
            "\n",
            "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
            "0                     1                       1      5  1303862400   \n",
            "1                     0                       0      1  1346976000   \n",
            "2                     1                       1      4  1219017600   \n",
            "3                     3                       3      2  1307923200   \n",
            "4                     0                       0      5  1350777600   \n",
            "\n",
            "                 Summary                                               Text  \n",
            "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
            "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
            "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
            "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
            "4            Great taffy  Great taffy at a great price.  There was a wid...  \n",
            "Total number of rows in the dataset: 568454\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd  # Library for working with data\n",
        "\n",
        "# Try to load the dataset\n",
        "try:\n",
        "    # Reads the uploaded file into a pandas DataFrame\n",
        "    df = pd.read_csv('Reviews.csv', encoding='utf-8')\n",
        "\n",
        "    # Show the first 5 rows\n",
        "    print(df.head())\n",
        "\n",
        "    # Display the total number of rows\n",
        "    print(\"Total number of rows in the dataset:\", df.shape[0])\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"The file was not found. Check if the file is uploaded correctly and the name is typed correctly.\")\n",
        "except Exception as e:\n",
        "    print(\"An error occurred:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KsCxhM1_kkO"
      },
      "source": [
        "## Check the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvWLDzfq_Ndn"
      },
      "source": [
        "Data Integrity Check: Identifying and Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsS0XIqJ_NI3",
        "outputId": "fb431a87-a01a-49e5-d213-3774f9d97dd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Id                         0\n",
            "ProductId                  0\n",
            "UserId                     0\n",
            "ProfileName               26\n",
            "HelpfulnessNumerator       0\n",
            "HelpfulnessDenominator     0\n",
            "Score                      0\n",
            "Time                       0\n",
            "Summary                   27\n",
            "Text                       0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values in each column\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Display the count of missing values for each column\n",
        "print(missing_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO0-w0VzFGoN"
      },
      "source": [
        "Data Cleansing: Handling Missing Values in Review Data.\n",
        "\n",
        "As we can see we have 26 ProfileName rows missing, and 27 summaries, we are going to remove those.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2G0rPQ-FGV-",
        "outputId": "6e94a0d0-3f59-44bf-962b-e7f068b86782"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New number of rows after removing missing values: 568401\n",
            "Id                        0\n",
            "ProductId                 0\n",
            "UserId                    0\n",
            "ProfileName               0\n",
            "HelpfulnessNumerator      0\n",
            "HelpfulnessDenominator    0\n",
            "Score                     0\n",
            "Time                      0\n",
            "Summary                   0\n",
            "Text                      0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Remove rows where any of these columns have missing values\n",
        "df_cleaned = df.dropna(subset=['ProfileName', 'Summary'])\n",
        "\n",
        "# Check the shape of the new DataFrame to see how many rows are left\n",
        "print(\"New number of rows after removing missing values:\", df_cleaned.shape[0])\n",
        "\n",
        "# Optionally, check again for missing values to ensure they're all handled\n",
        "print(df_cleaned.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSMtob_kGndk"
      },
      "source": [
        "Data Preprocessing: Streamlining and Cleansing the Review Datase\n",
        "\n",
        "Now, we will focus on removing columns that are not essential for our sentiment analysis. Specifically, we will exclude columns such as\n",
        "- Id\n",
        "- ProductId\n",
        "- UserId\n",
        "- ProfileName\n",
        "- HelpfulnessNumerator\n",
        "- HelpfulnessDenominator\n",
        "- Time\n",
        "- Score\n",
        "\n",
        "These columns are primarily identifiers and metadata that do not contribute to the sentiment analysis process. We will retain only the Summary and Text columns, as these contain the actual review content which is crucial for analyzing and understanding user sentiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZno6GCwGn19",
        "outputId": "bfdbc5c0-1530-45f1-c161-f708d06486bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows after cleaning: 568401\n"
          ]
        }
      ],
      "source": [
        "# Columns to keep that are likely useful for sentiment analysis\n",
        "columns_to_keep = ['Score', 'Summary', 'Text']\n",
        "\n",
        "# Reducing the DataFrame to only necessary columns\n",
        "df_reduced = df_cleaned[columns_to_keep]\n",
        "\n",
        "# Check the cleaned data structure\n",
        "print(\"Total rows after cleaning:\", df_cleaned.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRRFgpSrAG7A"
      },
      "source": [
        "Transform Ratings into Sentiment Categories:\n",
        "- Positive: Ratings of 4 or 5\n",
        "- Neutral: A rating of 3\n",
        "- Negative: Ratings of 1 or 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2A-Wp7rAHN8",
        "outputId": "ec9b47cb-6d82-4bda-ff3c-9e83f9ad32bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Score Sentiment\n",
            "0      5  Positive\n",
            "1      1  Negative\n",
            "2      4  Positive\n",
            "3      2  Negative\n",
            "4      5  Positive\n",
            "5      4  Positive\n",
            "6      5  Positive\n",
            "7      5  Positive\n",
            "8      5  Positive\n",
            "9      5  Positive\n",
            "Total rows after cleaning: 568401\n"
          ]
        }
      ],
      "source": [
        "# Explicitly create a copy of the reduced DataFrame\n",
        "df_reduced = df_cleaned[columns_to_keep].copy()\n",
        "\n",
        "# Function to categorize ratings\n",
        "def categorize_rating(score):\n",
        "    if score >= 4:\n",
        "        return 'Positive'\n",
        "    elif score == 3:\n",
        "        return 'Neutral'\n",
        "    else:\n",
        "        return 'Negative'\n",
        "\n",
        "# Apply function to the 'Score' column\n",
        "df_reduced['Sentiment'] = df_reduced['Score'].apply(categorize_rating)\n",
        "\n",
        "# Print the first 10 rows to see the original scores and their corresponding sentiments\n",
        "print(df_reduced[['Score', 'Sentiment']].head(10))\n",
        "print(\"Total rows after cleaning:\", df_reduced.shape[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-mRexKTKdKP"
      },
      "source": [
        "# 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMCXvXo8LtXj"
      },
      "source": [
        "Drop the Score Column, now that we have the sentiment, we dont need it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppbTd3aLKgrv"
      },
      "outputs": [],
      "source": [
        "#Dropping the 'Score' column from the DataFrame\n",
        "df_reduced.drop(columns=['Score'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p-hk3sbL6aw"
      },
      "source": [
        "## Comprehensive Text Cleaning and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVviJtwz2kwX"
      },
      "source": [
        "This section of the code is designed to clean and preprocess the text data from your dataset. It involves several steps that prepare the text for further NLP tasks such as sentiment analysis. The cleaning function will remove HTML tags, URLs, special characters, convert text to lowercase, and remove stopwords. We apply this function to both the Summary and Text columns to ensure they are uniformly cleaned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag-2YRPXL41z",
        "outputId": "ee9a9cf1-ed02-4f42-95f8-4e2f7fb7d10d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Summary        Cleaned_Summary  \\\n",
            "0  Good Quality Dog Food  good quality dog food   \n",
            "1      Not as Advertised             advertised   \n",
            "2  \"Delight\" says it all           delight says   \n",
            "3         Cough Medicine         cough medicine   \n",
            "4            Great taffy            great taffy   \n",
            "\n",
            "                                                Text  \\\n",
            "0  I have bought several of the Vitality canned d...   \n",
            "1  Product arrived labeled as Jumbo Salted Peanut...   \n",
            "2  This is a confection that has been around a fe...   \n",
            "3  If you are looking for the secret ingredient i...   \n",
            "4  Great taffy at a great price.  There was a wid...   \n",
            "\n",
            "                                        Cleaned_Text  \n",
            "0  bought several vitality canned dog food produc...  \n",
            "1  product arrived labeled jumbo salted peanutsth...  \n",
            "2  confection around centuries light pillowy citr...  \n",
            "3  looking secret ingredient robitussin believe f...  \n",
            "4  great taffy great price wide assortment yummy ...  \n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import re\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define a comprehensive cleaning function\n",
        "def clean_and_preprocess(text):\n",
        "    # Check if the input is valid HTML (optional but recommended)\n",
        "    if '<' in text and '>' in text:\n",
        "        text = BeautifulSoup(text, \"lxml\").get_text()\n",
        "    else:\n",
        "        text = text  # Skip BeautifulSoup if it’s not valid HTML\n",
        "\n",
        "    # Remove URLs and special characters\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # URLs\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Special characters\n",
        "\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower().strip()\n",
        "\n",
        "    # Remove stopwords\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# Handle null values and apply the cleaning function\n",
        "# Check for and handle null values\n",
        "df_reduced['Summary'] = df_reduced['Summary'].fillna('')\n",
        "df_reduced['Text'] = df_reduced['Text'].fillna('')\n",
        "\n",
        "# Apply the comprehensive cleaning and preprocessing function\n",
        "df_reduced['Cleaned_Summary'] = df_reduced['Summary'].apply(clean_and_preprocess)\n",
        "df_reduced['Cleaned_Text'] = df_reduced['Text'].apply(clean_and_preprocess)\n",
        "\n",
        "# Display the cleaned text to verify\n",
        "print(df_reduced[['Summary', 'Cleaned_Summary', 'Text', 'Cleaned_Text']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-ON0m9F8qc9"
      },
      "source": [
        "## Text Vectorization Using TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joUqJrXY8shX"
      },
      "source": [
        "This section of the code is focused on transforming the preprocessed text into a numerical format that machine learning models can interpret and analyze. Using the TF-IDF (Term Frequency-Inverse Document Frequency) vectorizer, we convert the cleaned text from the 'Summary' and 'Text' columns into a set of numerical features. TF-IDF measures not just the frequency of words in each document (text entry), but adjusts this frequency against the number of documents the words appear in, which helps to highlight words that are more important to the specific document. This is crucial for tasks like sentiment analysis or topic modeling where the significance of words plays a key role in understanding the content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEiyvBFj6Rv7",
        "outputId": "975d6ffd-980c-465a-8aa0-7c85fa929664"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Summary shape: (568401, 41201)\n",
            "TF-IDF Text shape: (568401, 307176)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the cleaned text\n",
        "tfidf_summary = tfidf_vectorizer.fit_transform(df_reduced['Cleaned_Summary'])\n",
        "tfidf_text = tfidf_vectorizer.fit_transform(df_reduced['Cleaned_Text'])\n",
        "\n",
        "# Optionally, display the shape of the vectorized data\n",
        "print(\"TF-IDF Summary shape:\", tfidf_summary.shape)\n",
        "print(\"TF-IDF Text shape:\", tfidf_text.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrFH2fYjAAJN"
      },
      "source": [
        "## Efficient Replacement of Common Terms with Count Tracking\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code efficiently replaces predefined common terms (e.g., abbreviations, contractions, and informal expressions) in the Cleaned_Summary and Cleaned_Text columns of the dataset with their standardized or expanded forms. It also tracks and reports the total number of replacements made.\n",
        "\n",
        "Key Features:\n",
        "\n",
        "- Regex-Based Replacement: Uses a compiled regular expression pattern for fast and memory-efficient replacements, ensuring only whole words are replaced.\n",
        "- Replacement and Counting: Tracks the number of replacements for each column during the replacement process.\n",
        "- Optimized Performance: Minimizes memory usage by combining replacement and counting into a single operation."
      ],
      "metadata": {
        "id": "GJbhLOK8GhrF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eifBpQwWAAdt"
      },
      "outputs": [],
      "source": [
        "# Define a dictionary for common terms and their replacements\n",
        "replacement_dict = {\n",
        "    \"e.g.\": \"for example\",\n",
        "    \"i.e.\": \"that is\",\n",
        "    \"etc.\": \"and so on\",\n",
        "    \"Dr.\": \"Doctor\",\n",
        "    \"vs.\": \"versus\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"n't\": \" not\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"lol\": \"laugh out loud\",\n",
        "    \"omg\": \"oh my god\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Convert the replacement dictionary into a regex pattern for efficient replacement\n",
        "pattern = re.compile(r'\\b(' + '|'.join(re.escape(term) for term in replacement_dict.keys()) + r')\\b')\n",
        "\n",
        "def replace_and_count_changes(text):\n",
        "    \"\"\"\n",
        "    Replace terms in the text and count the number of replacements made.\n",
        "    \"\"\"\n",
        "    # Initialize count of changes\n",
        "    changes = 0\n",
        "\n",
        "    # Function to replace and count replacements\n",
        "    def replacement_function(match):\n",
        "        nonlocal changes\n",
        "        changes += 1\n",
        "        return replacement_dict[match.group(0)]\n",
        "\n",
        "    # Apply the replacement using regex\n",
        "    replaced_text = pattern.sub(replacement_function, text)\n",
        "    return replaced_text, changes\n",
        "\n",
        "# Initialize counters\n",
        "summary_changes_total = 0\n",
        "text_changes_total = 0\n",
        "\n",
        "# Apply replacements and count changes for 'Cleaned_Summary'\n",
        "df_reduced['Cleaned_Summary'], summary_changes_col = zip(*df_reduced['Cleaned_Summary'].apply(replace_and_count_changes))\n",
        "summary_changes_total = sum(summary_changes_col)\n",
        "\n",
        "# Apply replacements and count changes for 'Cleaned_Text'\n",
        "df_reduced['Cleaned_Text'], text_changes_col = zip(*df_reduced['Cleaned_Text'].apply(replace_and_count_changes))\n",
        "text_changes_total = sum(text_changes_col)\n",
        "\n",
        "# Total changes\n",
        "total_changes = summary_changes_total + text_changes_total\n",
        "print(f\"Total replacements made: {total_changes}\")\n",
        "print(f\"Replacements in 'Cleaned_Summary': {summary_changes_total}\")\n",
        "print(f\"Replacements in 'Cleaned_Text': {text_changes_total}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAHWhRikF1rc",
        "outputId": "d91953e7-dffb-443c-b98f-3ff64e9eb0f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total replacements made: 3578\n",
            "Replacements in 'Cleaned_Summary': 596\n",
            "Replacements in 'Cleaned_Text': 2982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3"
      ],
      "metadata": {
        "id": "KYoVXTfxHEgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Model Development for Sentiment Analysis"
      ],
      "metadata": {
        "id": "fleyNfVrHFkn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task, we'll:\n",
        "\n",
        "1. Split the dataset into training and testing sets using a 70-30 split.\n",
        "Vectorize the Cleaned_Text column using TF-IDF to prepare it for machine learning models.\n",
        "2. Train and evaluate two baseline models:\n",
        "3. Logistic Regression: A simple, effective classifier.\n",
        "4. Support Vector Machine (SVM): Known for its robustness in text classification.\n",
        "5. Compare their performance using metrics like accuracy, precision, recall, and F1 score."
      ],
      "metadata": {
        "id": "RFCUI33QJ609"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Data Splitting"
      ],
      "metadata": {
        "id": "j1OignjEKDsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data\n",
        "X = df_reduced['Cleaned_Text']  # Feature: Cleaned_Text\n",
        "y = df_reduced['Sentiment']     # Target: Sentiment\n",
        "\n",
        "# Train-test split (70-30)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Testing set size: {X_test.shape[0]}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1imcZgy0HF7l",
        "outputId": "eb14fc36-b5cd-4f12-f63c-f0673c0528b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 397880\n",
            "Testing set size: 170521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Text Vectorization with TF-IDF"
      ],
      "metadata": {
        "id": "Kw2ngmoaKH-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Using 5000 features for efficiency\n",
        "\n",
        "# Fit and transform training data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform testing data\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "print(\"TF-IDF vectorization complete.\")\n",
        "print(f\"Training data shape: {X_train_tfidf.shape}\")\n",
        "print(f\"Testing data shape: {X_test_tfidf.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrIdXkETKISC",
        "outputId": "9674ff38-eb23-4586-b851-88584a96606f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF vectorization complete.\n",
            "Training data shape: (397880, 5000)\n",
            "Testing data shape: (170521, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Logistic Regression Model"
      ],
      "metadata": {
        "id": "cufgvtfHKl0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Initialize and train Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "logreg.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = logreg.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Classification Report for Logistic Regression:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjNzmhCKKmQq",
        "outputId": "9afb93d5-513d-44d4-f826-55c6e76a730b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for Logistic Regression:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.74      0.67      0.70     24366\n",
            "     Neutral       0.52      0.18      0.27     12835\n",
            "    Positive       0.90      0.97      0.93    133320\n",
            "\n",
            "    accuracy                           0.87    170521\n",
            "   macro avg       0.72      0.61      0.63    170521\n",
            "weighted avg       0.85      0.87      0.85    170521\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 16280    974   7112]\n",
            " [  2768   2371   7696]\n",
            " [  3089   1254 128977]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Support Vector Machine (SVM) Model"
      ],
      "metadata": {
        "id": "OLCesX3dK-cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# Initialize and train SVM model\n",
        "svm = LinearSVC(random_state=42)\n",
        "svm.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_svm = svm.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Classification Report for SVM:\")\n",
        "print(classification_report(y_test, y_pred_svm))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_svm))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nK4RvHN6K-r3",
        "outputId": "1c5127c8-9426-490f-d843-0b8a6c744f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for SVM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.72      0.67      0.70     24366\n",
            "     Neutral       0.58      0.11      0.19     12835\n",
            "    Positive       0.89      0.97      0.93    133320\n",
            "\n",
            "    accuracy                           0.86    170521\n",
            "   macro avg       0.73      0.59      0.61    170521\n",
            "weighted avg       0.84      0.86      0.84    170521\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 16388    457   7521]\n",
            " [  3089   1462   8284]\n",
            " [  3133    589 129598]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EohBfBdtN2fi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPM9ihpaycC6wR5nXYlzK1/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}